{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gate:\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# Example of an AddGate class inheriting from the Gate class \n",
    "import numpy as np\n",
    "class AddGate(Gate): \n",
    "    def forward(self, x, y): \n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        return x + y \n",
    "    def backward(self, dz): \n",
    "        print(dz.shape)\n",
    "        print(\"add\")\n",
    "        print(self.y.shape)\n",
    "        print(self.x.shape)\n",
    "        dx = dz * np.ones_like(self.x) \n",
    "        dy = dz * np.ones_like(self.y) \n",
    "        return dy \n",
    "    # Example of a MultiplyGate class inheriting from the Gate class\n",
    "import numpy as np\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return np.dot(x, y)\n",
    "\n",
    "    def backward(self, dz):\n",
    "        print(\"multipllly\")\n",
    "        # print(dz.shape)\n",
    "        print(self.y.shape)\n",
    "        print(self.x.shape)\n",
    "        dz=dz.reshape(self.x.shape[0],1)\n",
    "        self.y=self.y.reshape(self.y.shape[0],1)\n",
    "        \n",
    "        dx = np.dot(dz, self.y.T)\n",
    "        dy = np.dot(self.x.T, dz)\n",
    "        # print(dx.shape)\n",
    "        # print(dy.shape)\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Softmax activation function\n",
    "class SoftmaxActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def backward(self, dz):\n",
    "        softmax_x = self.forward(self.x)\n",
    "        dx = dz * softmax_x * (1 - softmax_x)\n",
    "        return dx\n",
    "# Example of a Sigmoid activation function\n",
    "class SigmoidActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, dz):\n",
    "        sigmoid_x = 1 / (1 + np.exp(-self.x))\n",
    "        # print(\"in Sigmoid activation\")\n",
    "        # print(dz.shape)\n",
    "        # print(self.x.shape)\n",
    "        # print(sigmoid_x.shape)\n",
    "        dx = dz * sigmoid_x * (1 - sigmoid_x)\n",
    "        \n",
    "        # print(dx.shape)\n",
    "\n",
    "        return dx\n",
    "# Example of a ReLU activation function\n",
    "class ReLUActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "\n",
    "    def backward(self, dz):\n",
    "        dx = dz * np.where(self.x > 0, 1, 0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Binary Cross-Entropy (BCE) loss function\n",
    "class BinaryCrossEntropyLoss(Gate):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def backward(self):\n",
    "        dy_pred = - (self.y_true / self.y_pred) + ((1 - self.y_true) / (1 - self.y_pred))\n",
    "        return dy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of L2 loss function\n",
    "class L2Loss(Gate):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self,y_pred, y_true):\n",
    "        dx = y_pred - y_true\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,layers_dim, actiation_func, loss):\n",
    "        self.layers=[]\n",
    "        self.activFunc=[]\n",
    "        self.layers_dim = layers_dim\n",
    "        self.num_layers=len(layers_dim)\n",
    "        self.grads=[]\n",
    "        self.weightsgrads=[]\n",
    "        self.biasgrads=[]\n",
    "        if len(layers_dim)!=len(actiation_func)+1:\n",
    "            raise ValueError(\"the number of layers is not equal to the number of activation funcs\")\n",
    "        for i in range(0,len(layers_dim)-1):\n",
    "            layer=[]\n",
    "            layer.append(MultiplyGate())\n",
    "            layer.append(AddGate())\n",
    "            self.layers.append(layer)\n",
    "        for i in range(0,len(actiation_func)):\n",
    "            if actiation_func[i] =='sigmoid':\n",
    "                self.activFunc.append(SigmoidActivation())\n",
    "            \n",
    "            if actiation_func[i] =='relu':\n",
    "                self.activFunc.append(ReLUActivation())\n",
    "            \n",
    "            if actiation_func[i] =='softmax':\n",
    "                self.activFunc.append(SoftmaxActivation())\n",
    "            # print(type(self.activFunc[i]))\n",
    "        if loss=='CE':\n",
    "            self.loss=BinaryCrossEntropyLoss()\n",
    "        elif loss=='L2':\n",
    "            self.loss=L2Loss()\n",
    "        self.parameters=self.initialise_parameters()\n",
    "        # print(len(self.parameters))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def initialise_parameters(self):\n",
    "        parameters = []\n",
    "        for i in range(1, len(self.layers_dim)):\n",
    "            weights = np.random.rand(self.layers_dim[i],self.layers_dim[i-1]) \n",
    "            biases = np.random.rand(self.layers_dim[i])\n",
    "            layer_param = {'weights': weights, 'biases': biases}\n",
    "            parameters.append(layer_param)\n",
    "            # print((parameters[i-1]['weights'].shape))\n",
    "            # print((parameters[i-1]['biases'].shape))\n",
    "        return parameters\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(0,self.num_layers-1):\n",
    "\n",
    "            x = self.layers[i][0].forward( self.parameters[i]['weights'],x)\n",
    "            # print(x.shape)\n",
    "            x=x.flatten()\n",
    "            x = self.layers[i][1].forward(x, self.parameters[i]['biases'])\n",
    "            # print(x.shape)\n",
    "            # print(\"actx,bef\",x.shape)\n",
    "            x = self.activFunc[i].forward(x)\n",
    "\n",
    "            # print(\"actx,\",x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "\n",
    "\n",
    "    def backward(self,losscomputed):\n",
    "        dy=losscomputed\n",
    "        self.grads.append(dy)\n",
    "        dz=dy\n",
    "        # print(dy.shape)\n",
    "        for i in range(self.num_layers-1,0,-1):\n",
    "            # print(\"iter\",i)\n",
    "            dz=self.activFunc[i-1].backward(dz)\n",
    "            # print(\"aft\",dz.shape)\n",
    "            dz=dz.flatten()\n",
    "            # print(dz.shape)\n",
    "            if len(dz)==1:\n",
    "                dz=dz[0]\n",
    "            db=self.layers[i-1][1].backward(dz)\n",
    "            dw,dz=self.layers[i-1][0].backward(dz)\n",
    "            self.weightsgrads.append(dw)\n",
    "            # print(dz.shape)\n",
    "            dz=dz.flatten()\n",
    "            self.biasgrads.append(db)\n",
    "            \n",
    "            self.grads.append(dz)\n",
    "            # print(len(self.weightsgrads))\n",
    "            # print(len(self.biasgrads))\n",
    "        \n",
    "            \n",
    "\n",
    "    def train(self, X_train, Y_train, learning_rate, num_epochs, gradient_descent_method='batch',batch_size=None,patience=10):\n",
    "        ss=[]\n",
    "        for i in range (0,num_epochs):\n",
    "            \n",
    "            \n",
    "            if gradient_descent_method=='batch':\n",
    "                y_pred=self.predict(X_train)\n",
    "                loss=self.loss.forward(y_pred,Y_train)\n",
    "                # print(loss)\n",
    "                computed_loss=self.loss.backward()\n",
    "                computed_loss=np.array(computed_loss)\n",
    "                computed_loss=np.mean(computed_loss)\n",
    "                # print(computed_loss)\n",
    "                self.backward(computed_loss)\n",
    "                \n",
    "                self.update_parameters(learning_rate)\n",
    "                ss.append(loss)\n",
    "\n",
    "            if gradient_descent_method=='stocastic_batch':\n",
    "                for j in range (0,len(X_train)):\n",
    "                    y_pred=self.predict(X_train[j])\n",
    "                    loss=self.loss.forward(y_pred,Y_train[j])\n",
    "                    # print(loss)\n",
    "                    computed_loss=self.loss.backward()\n",
    "                    computed_loss=np.array(computed_loss)\n",
    "                    computed_loss=np.mean(computed_loss)\n",
    "                    # print(computed_loss)\n",
    "                    self.backward(computed_loss)\n",
    "                    \n",
    "                    self.update_parameters(learning_rate)\n",
    "                    ss.append(loss)\n",
    "            if gradient_descent_method=='mini_batch':\n",
    "                for j in range (0,len(X_train)):\n",
    "                    y_pred=self.predict(X_train[j])\n",
    "                    loss=self.loss.forward(y_pred,Y_train[j])\n",
    "                    # print(loss)\n",
    "                    computed_loss=self.loss.backward()\n",
    "                    computed_loss=np.array(computed_loss)\n",
    "                    computed_loss=np.mean(computed_loss)\n",
    "                    # print(computed_loss)\n",
    "                    self.backward(computed_loss)\n",
    "                    \n",
    "                    self.update_parameters(learning_rate)\n",
    "                    ss.append(loss)\n",
    "        return ss\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "    def update_parameters(self,learning_rate):\n",
    "        parameters=[]\n",
    "        for i in range(0,len(self.parameters)):\n",
    "            w=self.parameters[i]['weights']-learning_rate*self.weightsgrads[len(self.parameters)-i-1]\n",
    "            \n",
    "                \n",
    "            # print(\"output\",self.parameters[i]['weights'].shape)\n",
    "            s=np.array(self.biasgrads[len(self.parameters)-i-1])\n",
    "            b=self.parameters[i]['biases']-learning_rate*s\n",
    "            layer_param = {'weights': w, 'biases': b}\n",
    "            parameters.append(layer_param)\n",
    "        self.parameters=parameters\n",
    "\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        y_pred=[]\n",
    "        for i in range(0,len(X)):\n",
    "            y=self.forward(X[i])\n",
    "            y_pred.append(y)\n",
    "            # print(y)\n",
    "        y_pred=np.array(y_pred)\n",
    "        # print(y_pred.shape)\n",
    "        return (y_pred)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"./training_data.csv\")\n",
    "y=dataset.iloc[0::,12:13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=dataset.iloc[::,1:10].values\n",
    "\n",
    "# print((len(X[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yehia_xd1awm4\\AppData\\Local\\Temp\\ipykernel_13368\\3532029100.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
      "C:\\Users\\yehia_xd1awm4\\AppData\\Local\\Temp\\ipykernel_13368\\3532029100.py:9: RuntimeWarning: divide by zero encountered in divide\n",
      "  dy_pred = - (self.y_true / self.y_pred) + ((1 - self.y_true) / (1 - self.y_pred))\n",
      "C:\\Users\\yehia_xd1awm4\\AppData\\Local\\Temp\\ipykernel_13368\\3719677229.py:24: RuntimeWarning: invalid value encountered in multiply\n",
      "  dx = dz * sigmoid_x * (1 - sigmoid_x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n",
      "()\n",
      "add\n",
      "(1,)\n",
      "(1,)\n",
      "(15,)\n",
      "(1, 15)\n",
      "(15,)\n",
      "add\n",
      "(15,)\n",
      "(15,)\n",
      "(30,)\n",
      "(15, 30)\n",
      "(30,)\n",
      "add\n",
      "(30,)\n",
      "(30,)\n",
      "(50,)\n",
      "(30, 50)\n",
      "(50,)\n",
      "add\n",
      "(50,)\n",
      "(50,)\n",
      "(9,)\n",
      "(50, 9)\n"
     ]
    }
   ],
   "source": [
    "dim=[len(X[0]),50,30,15,1]\n",
    "fun=['relu','sigmoid','relu','sigmoid']\n",
    "\n",
    "k=Model(dim,fun,'CE')\n",
    "y=np.random.randint(low=0 ,high=1,size=(569))\n",
    "# print(y.shape)\n",
    "k.train(X,y,num_epochs=30,learning_rate=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
