{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gate:\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# Example of an AddGate class inheriting from the Gate class \n",
    "import numpy as np\n",
    "class AddGate(Gate): \n",
    "    def forward(self, x, y): \n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        return x + y \n",
    "    def backward(self, dz): \n",
    "        dx = dz * np.ones_like(self.x) \n",
    "        dy = dz * np.ones_like(self.y) \n",
    "        return dx, dy \n",
    "    # Example of a MultiplyGate class inheriting from the Gate class\n",
    "import numpy as np\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return np.dot(x, y)\n",
    "\n",
    "    def backward(self, dz):\n",
    "        # print(\"multipllly\")\n",
    "        # print(dz.shape)\n",
    "        # print(self.y.shape)\n",
    "        # print(self.x.shape)\n",
    "        dz=dz.reshape(self.x.shape[0],1)\n",
    "        self.y=self.y.reshape(self.y.shape[0],1)\n",
    "        \n",
    "        dx = np.dot(dz, self.y.T)\n",
    "        dy = np.dot(self.x.T, dz)\n",
    "        # print(dx.shape)\n",
    "        # print(dy.shape)\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Softmax activation function\n",
    "class SoftmaxActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def backward(self, dz):\n",
    "        softmax_x = self.forward(self.x)\n",
    "        dx = dz * softmax_x * (1 - softmax_x)\n",
    "        return dx\n",
    "# Example of a Sigmoid activation function\n",
    "class SigmoidActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, dz):\n",
    "        sigmoid_x = 1 / (1 + np.exp(-self.x))\n",
    "        # print(\"in Sigmoid activation\")\n",
    "        # print(dz.shape)\n",
    "        # print(self.x.shape)\n",
    "        # print(sigmoid_x.shape)\n",
    "        dx = dz * sigmoid_x * (1 - sigmoid_x)\n",
    "        \n",
    "        print(dx.shape)\n",
    "\n",
    "        return dx\n",
    "# Example of a ReLU activation function\n",
    "class ReLUActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "\n",
    "    def backward(self, dz):\n",
    "        dx = dz * np.where(self.x > 0, 1, 0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Binary Cross-Entropy (BCE) loss function\n",
    "class BinaryCrossEntropyLoss(Gate):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def backward(self,y_pred, y_true):\n",
    "        dx = (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of L2 loss function\n",
    "class L2Loss(Gate):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self,y_pred, y_true):\n",
    "        dx = y_pred - y_true\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,layers_dim, actiation_func, loss):\n",
    "        self.layers=[]\n",
    "        self.activFunc=[]\n",
    "        self.layers_dim = layers_dim\n",
    "        self.num_layers=len(layers_dim)\n",
    "        self.grads=[]\n",
    "        self.weightsgrads=[]\n",
    "        self.biasgrads=[]\n",
    "        if len(layers_dim)!=len(actiation_func)+1:\n",
    "            raise ValueError(\"the number of layers is not equal to the number of activation funcs\")\n",
    "        for i in range(0,len(layers_dim)-1):\n",
    "            layer=[]\n",
    "            layer.append(MultiplyGate())\n",
    "            layer.append(AddGate())\n",
    "            self.layers.append(layer)\n",
    "        for i in range(0,len(actiation_func)):\n",
    "            if actiation_func[i] =='sigmoid':\n",
    "                self.activFunc.append(SigmoidActivation())\n",
    "            \n",
    "            if actiation_func[i] =='relu':\n",
    "                self.activFunc.append(ReLUActivation())\n",
    "            \n",
    "            if actiation_func[i] =='softmax':\n",
    "                self.activFunc.append(SoftmaxActivation())\n",
    "            # print(type(self.activFunc[i]))\n",
    "        if loss=='CE':\n",
    "            self.loss=BinaryCrossEntropyLoss()\n",
    "        elif loss=='L2':\n",
    "            self.loss=L2Loss()\n",
    "        self.parameters=self.initialise_parameters()\n",
    "        # print(len(self.parameters))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def initialise_parameters(self):\n",
    "        parameters = []\n",
    "        for i in range(1, len(self.layers_dim)):\n",
    "            weights = np.random.randint(low=0,high=3, size=(self.layers_dim[i],self.layers_dim[i-1])) \n",
    "            biases = np.random.randint(low=0,high=3, size=(self.layers_dim[i])) \n",
    "            layer_param = {'weights': weights, 'biases': biases}\n",
    "            parameters.append(layer_param)\n",
    "            # print((parameters[i-1]['weights'].shape))\n",
    "            # print((parameters[i-1]['biases'].shape))\n",
    "        return parameters\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(0,self.num_layers-1):\n",
    "            x = self.layers[i][0].forward( self.parameters[i]['weights'],x)\n",
    "            # print(x.shape)\n",
    "            x=x.flatten()\n",
    "            x = self.layers[i][1].forward(x, self.parameters[i]['biases'])\n",
    "            # print(x.shape)\n",
    "            # print(\"actx,bef\",x.shape)\n",
    "            x = self.activFunc[i].forward(x)\n",
    "\n",
    "            # print(\"actx,\",x)\n",
    "\n",
    "        return(x)\n",
    "        \n",
    "\n",
    "\n",
    "    def backward(self,losscomputed):\n",
    "        dy=losscomputed\n",
    "        self.grads.append(dy)\n",
    "        dz=dy\n",
    "        # print(dy.shape)\n",
    "        for i in range(self.num_layers-1,0,-1):\n",
    "            print(\"iter\",i)\n",
    "            dz=self.activFunc[i-1].backward(dz)\n",
    "            # print(\"aft\",dz.shape)\n",
    "            dz=dz.flatten()\n",
    "            # print(dz.shape)\n",
    "            if len(dz)==1:\n",
    "                dz=dz[0]\n",
    "            dw,dz=self.layers[i-1][0].backward(dz)\n",
    "            db=self.layers[i-1][1].backward(dz)\n",
    "            self.weightsgrads.append(dw)\n",
    "            # print(dz.shape)\n",
    "            dz=dz.flatten()\n",
    "            self.biasgrads.append(db)\n",
    "            # print(len(self.weightsgrads))\n",
    "            # print(len(self.biasgrads))\n",
    "        \n",
    "            \n",
    "\n",
    "    def train(self, X_train, Y_train, learning_rate, num_epochs, gradient_descent_method='batch',batch_size=None,patience=10):\n",
    "        for i in range (0,num_epochs):\n",
    "            \n",
    "            \n",
    "            if gradient_descent_method=='batch':\n",
    "                y_pred=self.predict(X_train)\n",
    "                loss=self.loss.forward(y_pred,Y_train)\n",
    "                print(loss)\n",
    "                computed_loss=self.loss.backward(loss,Y_train)\n",
    "                computed_loss=np.array(computed_loss)\n",
    "                computed_loss=np.mean(computed_loss)\n",
    "                print(computed_loss)\n",
    "                self.backward(computed_loss)\n",
    "                self.update_parameters(learning_rate)\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "\n",
    "    def update_parameters(self,learning_rate):\n",
    "        for i in range(0,len(self.parameters)):\n",
    "            self.parameters[i]['weights']=self.parameters[i]['weights']-learning_rate*self.weightsgrads[i]\n",
    "            bias_grads_array = np.array(self.biasgrads[i])\n",
    "            # Perform the multiplication with the learning rate\n",
    "            self.parameters[i]['biases'] = self.parameters[i]['biases'] - learning_rate * bias_grads_array\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        y_pred=[]\n",
    "        for i in range(0,len(X)):\n",
    "            y=self.forward(X[i])\n",
    "            y_pred.append(y)\n",
    "            # print(y)\n",
    "        y_pred=np.array(y_pred)\n",
    "        print(y_pred.shape)\n",
    "        return (y_pred)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"./training_data.csv\")\n",
    "y=dataset.iloc[0:100:,4:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X=dataset.iloc[::,3:4].values\n",
    "\n",
    "print((len(X[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n",
      "(111479, 1)\n",
      "0.5\n",
      "0.5\n",
      "iter 3\n",
      "(1,)\n",
      "iter 2\n",
      "iter 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (8,3) and (1,) not aligned: 3 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m y\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m ,high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m569\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 97\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, X_train, Y_train, learning_rate, num_epochs, gradient_descent_method, batch_size, patience)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m0\u001b[39m,num_epochs):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gradient_descent_method\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m         y_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mforward(y_pred,Y_train)\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "Cell \u001b[1;32mIn[11], line 121\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    119\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(X)):\n\u001b[1;32m--> 121\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     y_pred\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# print(y)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 54\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 54\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m         x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mflatten()\n",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m, in \u001b[0;36mMultiplyGate.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (8,3) and (1,) not aligned: 3 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "dim=[len(X[0]),8,3,1]\n",
    "fun=['relu','relu','sigmoid']\n",
    "\n",
    "k=Model(dim,fun,'L2')\n",
    "y=np.random.randint(low=0 ,high=1,size=(569))\n",
    "print(y.shape)\n",
    "k.train(X,y,num_epochs=20,learning_rate=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
