{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base class for Gates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the base class for Gates \n",
    "class Gate: \n",
    "    def forward(self): \n",
    "        raise NotImplementedError \n",
    "    def backward(self): \n",
    "        raise NotImplementedError  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AddGate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an AddGate class inheriting from the Gate class \n",
    "class AddGate(Gate): \n",
    "    def forward(self, x, y): \n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        return x + y \n",
    "    def backward(self, dz): \n",
    "        dx = dz * np.ones_like(self.x) \n",
    "        dy = dz * np.ones_like(self.y) \n",
    "        return dx, dy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MultiplyGate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a MultiplyGate class inheriting from the Gate class\n",
    "class MultiplyGate(Gate):\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "\n",
    "    def backward(self, dz):\n",
    "        dx = dz * self.y\n",
    "        dy = dz * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of a Linear activation function\n",
    "class LinearActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, dz):\n",
    "        dx = dz\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a ReLU activation function\n",
    "class ReLUActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, dz):\n",
    "        dx = dz * np.where(self.x > 0, 1, 0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class SigmoidActivation:\n",
    "#     def __init__(self):\n",
    "#         self.x = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.x = x\n",
    "#         return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#     def backward(self, dz):\n",
    "#         sigmoid_x = 1 / (1 + np.exp(-self.x))\n",
    "#         dx = dz * sigmoid_x * (1 - sigmoid_x)\n",
    "#         return dx\n",
    "class SigmoidActivation:\n",
    "    def __init__(self):\n",
    "        self.sigmoid_x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sigmoid_x = 1 / (1 + np.exp(-x))\n",
    "        return self.sigmoid_x\n",
    "\n",
    "    def backward(self, dz):\n",
    "        if self.sigmoid_x is None:\n",
    "            raise ValueError(\"Forward method must be called before backward method.\")\n",
    "        dx = dz * self.sigmoid_x * (1 - self.sigmoid_x)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Softmax activation function\n",
    "class SoftmaxActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def backward(self, dz):\n",
    "        softmax_x = self.forward(self.x)\n",
    "        dx = dz * softmax_x * (1 - softmax_x)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Tanh activation function\n",
    "class TanhActivation(Gate):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, dz):\n",
    "        tanh_x = np.tanh(self.x)\n",
    "        dx = dz * (1 - tanh_x ** 2)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Binary Cross-Entropy (BCE) loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Binary Cross-Entropy (BCE) loss function\n",
    "class BinaryCrossEntropyLoss(Gate):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def backward(self):\n",
    "        dx = (self.y_pred - self.y_true) / (self.y_pred * (1 - self.y_pred))\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of L2 loss function\n",
    "class L2Loss(Gate):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self):\n",
    "        dx = self.y_pred - self.y_true\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Computational Graph / Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers_dim, activation_func, loss):\n",
    "        self.layers_dim = layers_dim\n",
    "        self.activation_func = activation_func\n",
    "        self.loss = loss\n",
    "        self.parameters = {}\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        # Initialize weights and biases using Xavier initialization\n",
    "        for i in range(1, len(layers_dim)):\n",
    "            prev_dim = layers_dim[i - 1]\n",
    "            curr_dim = layers_dim[i]\n",
    "            self.parameters[\"W\" + str(i)] = np.random.randn(curr_dim, prev_dim) * np.sqrt(1 / prev_dim)\n",
    "            self.parameters[\"b\" + str(i)] = np.zeros((curr_dim, 1))  # Initialize biases as column vectors\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.activations[\"A0\"] = X\n",
    "\n",
    "        for i in range(1, len(self.layers_dim)):\n",
    "            prev_a = self.activations[\"A\" + str(i - 1)]\n",
    "            W = self.parameters[\"W\" + str(i)]\n",
    "            b = self.parameters[\"b\" + str(i)]\n",
    "\n",
    "            activation_func = self.activation_func()  # Create an instance of the activation function\n",
    "            Z = np.dot(W, prev_a) + b\n",
    "            A = activation_func.forward(Z)\n",
    "\n",
    "            self.activations[\"A\" + str(i)] = A\n",
    "            self.activations[\"Z\" + str(i)] = Z\n",
    "\n",
    "        return self.activations[\"A\" + str(len(self.layers_dim) - 1)]\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Forward propagation\n",
    "            A = self.predict(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.loss.forward(A, y)\n",
    "            mse = np.mean((A - y) ** 2)\n",
    "\n",
    "            # Backward propagation\n",
    "            dA = self.loss.backward()\n",
    "            self.gradients[\"dA\" + str(len(self.layers_dim) - 1)] = dA\n",
    "\n",
    "            for i in reversed(range(1, len(self.layers_dim))):\n",
    "                activation_func = self.activation_func()  # Create an instance of the activation function\n",
    "                A_prev = self.activations[\"A\" + str(i - 1)]\n",
    "                W = self.parameters[\"W\" + str(i)]\n",
    "                b = self.parameters[\"b\" + str(i)]\n",
    "\n",
    "                Z = self.activations[\"Z\" + str(i)]  # Retrieve Z from stored activations\n",
    "                activation_func.forward(Z)  # Call forward method to compute self.sigmoid_x\n",
    "                dZ = activation_func.backward(self.gradients[\"dA\" + str(i)])  # Pass Z to backward method\n",
    "\n",
    "                dW = np.dot(dZ, A_prev.T)\n",
    "                db = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "                self.gradients[\"dA\" + str(i - 1)] = np.dot(W.T, dZ)\n",
    "                self.gradients[\"dW\" + str(i)] = dW\n",
    "                self.gradients[\"db\" + str(i)] = db\n",
    "\n",
    "            # Update parameters\n",
    "            for i in range(1, len(self.layers_dim)):\n",
    "                self.parameters[\"W\" + str(i)] -= learning_rate * self.gradients[\"dW\" + str(i)]\n",
    "                self.parameters[\"b\" + str(i)] -= learning_rate * self.gradients[\"db\" + str(i)]\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss}, MSE: {mse}\")\n",
    "            print(\"Weights:\")\n",
    "            for i in range(1, len(self.layers_dim)):\n",
    "                print(f\"Layer {i}:\")\n",
    "                print(f\"W{i}:\")\n",
    "                print(self.parameters[\"W\" + str(i)])\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Example usage\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.13048606660432394, MSE: 0.26097213320864787\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4843833   0.71454329]\n",
      " [ 0.05071271 -0.37626411]\n",
      " [-0.18416556 -0.15371826]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.56295672  0.18003536 -0.02740132]]\n",
      "\n",
      "Epoch 2/100, Loss: 0.13001537012266154, MSE: 0.2600307402453231\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48418205  0.71425049]\n",
      " [ 0.05050028 -0.37649457]\n",
      " [-0.18413291 -0.15368523]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.55684282  0.17551582 -0.03192046]]\n",
      "\n",
      "Epoch 3/100, Loss: 0.1295804311074689, MSE: 0.2591608622149378\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48400456  0.71398133]\n",
      " [ 0.05030115 -0.37671159]\n",
      " [-0.1840963  -0.15364817]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.55097296  0.17117422 -0.03626414]]\n",
      "\n",
      "Epoch 4/100, Loss: 0.12917901450787012, MSE: 0.25835802901574023\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48384977  0.71373478]\n",
      " [ 0.05011452 -0.37691594]\n",
      " [-0.18405627 -0.15360764]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.5453408   0.16700588 -0.04043679]]\n",
      "\n",
      "Epoch 5/100, Loss: 0.12880894768235565, MSE: 0.2576178953647113\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48371665  0.71350983]\n",
      " [ 0.04993963 -0.37710838]\n",
      " [-0.18401335 -0.15356415]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.53993977  0.16300597 -0.04444306]]\n",
      "\n",
      "Epoch 6/100, Loss: 0.12846813232704352, MSE: 0.25693626465408703\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48360418  0.71330551]\n",
      " [ 0.04977576 -0.37728962]\n",
      " [-0.18396801 -0.15351817]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.53476312  0.15916952 -0.04828771]]\n",
      "\n",
      "Epoch 7/100, Loss: 0.1281545535876282, MSE: 0.2563091071752564\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48351137  0.71312086]\n",
      " [ 0.04962223 -0.37746033]\n",
      " [-0.18392067 -0.15347015]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.52980396  0.15549148 -0.05197559]]\n",
      "\n",
      "Epoch 8/100, Loss: 0.1278662866677107, MSE: 0.2557325733354214\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48343727  0.71295494]\n",
      " [ 0.04947839 -0.37762117]\n",
      " [-0.18387174 -0.15342047]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.52505533  0.15196675 -0.05551166]]\n",
      "\n",
      "Epoch 9/100, Loss: 0.12760150124799147, MSE: 0.25520300249598293\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48338096  0.71280685]\n",
      " [ 0.04934362 -0.37777275]\n",
      " [-0.18382156 -0.15336949]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.52051027  0.1485902  -0.05890086]]\n",
      "\n",
      "Epoch 10/100, Loss: 0.12735846402217976, MSE: 0.2547169280443595\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48334155  0.71267574]\n",
      " [ 0.04921733 -0.37791565]\n",
      " [-0.18377045 -0.15331754]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.51616177  0.14535673 -0.06214818]]\n",
      "\n",
      "Epoch 11/100, Loss: 0.12713553963927743, MSE: 0.25427107927855486\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48331818  0.71256075]\n",
      " [ 0.04909898 -0.37805041]\n",
      " [-0.18371871 -0.1532649 ]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.5120029   0.14226126 -0.06525856]]\n",
      "\n",
      "Epoch 12/100, Loss: 0.12693119032059824, MSE: 0.2538623806411965\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48331006  0.7124611 ]\n",
      " [ 0.04898804 -0.37817756]\n",
      " [-0.18366659 -0.15321184]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.5080268   0.13929874 -0.0682369 ]]\n",
      "\n",
      "Epoch 13/100, Loss: 0.1267439743955068, MSE: 0.2534879487910136\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48331638  0.71237601]\n",
      " [ 0.04888405 -0.37829758]\n",
      " [-0.18361431 -0.15315859]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.50422668  0.13646424 -0.07108804]]\n",
      "\n",
      "Epoch 14/100, Loss: 0.12657254397404816, MSE: 0.2531450879480963\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48333642  0.71230475]\n",
      " [ 0.04878653 -0.37841092]\n",
      " [-0.18356209 -0.15310535]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.50059589  0.13375288 -0.07381677]]\n",
      "\n",
      "Epoch 15/100, Loss: 0.1264156419486312, MSE: 0.2528312838972624\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48336946  0.71224663]\n",
      " [ 0.04869506 -0.37851802]\n",
      " [-0.1835101  -0.1530523 ]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4971279   0.1311599  -0.07642775]]\n",
      "\n",
      "Epoch 16/100, Loss: 0.1262720984916596, MSE: 0.2525441969833192\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48341483  0.71220096]\n",
      " [ 0.04860925 -0.37861927]\n",
      " [-0.1834585  -0.1529996 ]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.49381633  0.12868062 -0.07892557]]\n",
      "\n",
      "Epoch 17/100, Loss: 0.12614082719211606, MSE: 0.2522816543842321\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48347189  0.71216712]\n",
      " [ 0.04852872 -0.37871506]\n",
      " [-0.18340742 -0.15294739]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.49065496  0.12631051 -0.08131468]]\n",
      "\n",
      "Epoch 18/100, Loss: 0.12602082095201356, MSE: 0.2520416419040271\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48354003  0.71214451]\n",
      " [ 0.04845312 -0.37880573]\n",
      " [-0.18335698 -0.15289578]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.48763773  0.12404514 -0.08359945]]\n",
      "\n",
      "Epoch 19/100, Loss: 0.12591114774357115, MSE: 0.2518222954871423\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48361867  0.71213254]\n",
      " [ 0.04838213 -0.37889162]\n",
      " [-0.18330728 -0.15284489]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.48475877  0.12188023 -0.08578411]]\n",
      "\n",
      "Epoch 20/100, Loss: 0.1258109463100472, MSE: 0.2516218926200944\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48370727  0.71213067]\n",
      " [ 0.04831544 -0.37897302]\n",
      " [-0.1832584  -0.15279479]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.48201237  0.11981161 -0.08787275]]\n",
      "\n",
      "Epoch 21/100, Loss: 0.12571942187736168, MSE: 0.25143884375472336\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48380531  0.71213839]\n",
      " [ 0.04825277 -0.37905024]\n",
      " [-0.18321042 -0.15274557]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.47939301  0.11783526 -0.08986937]]\n",
      "\n",
      "Epoch 22/100, Loss: 0.12563584192988603, MSE: 0.25127168385977205\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4839123   0.7121552 ]\n",
      " [ 0.04819386 -0.37912353]\n",
      " [-0.1831634  -0.15269727]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.47689535  0.11594728 -0.09177781]]\n",
      "\n",
      "Epoch 23/100, Loss: 0.1255595320919462, MSE: 0.2511190641838924\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48402777  0.71218064]\n",
      " [ 0.04813846 -0.37919315]\n",
      " [-0.18311738 -0.15264996]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.47451425  0.11414391 -0.09360178]]\n",
      "\n",
      "Epoch 24/100, Loss: 0.12548987214651358, MSE: 0.25097974429302716\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48415129  0.71221427]\n",
      " [ 0.04808634 -0.37925932]\n",
      " [-0.18307241 -0.15260366]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.47224474  0.11242152 -0.09534488]]\n",
      "\n",
      "Epoch 25/100, Loss: 0.12542629221408375, MSE: 0.2508525844281675\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48428243  0.71225567]\n",
      " [ 0.04803729 -0.37932228]\n",
      " [-0.1830285  -0.15255842]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.47008201  0.11077663 -0.09701057]]\n",
      "\n",
      "Epoch 26/100, Loss: 0.12536826910768248, MSE: 0.25073653821536496\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48442082  0.71230445]\n",
      " [ 0.04799111 -0.37938222]\n",
      " [-0.18298568 -0.15251424]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.46802147  0.10920588 -0.09860218]]\n",
      "\n",
      "Epoch 27/100, Loss: 0.12531532287412903, MSE: 0.25063064574825805\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48456608  0.71236024]\n",
      " [ 0.04794761 -0.37943933]\n",
      " [-0.18294397 -0.15247116]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.46605869  0.10770603 -0.10012292]]\n",
      "\n",
      "Epoch 28/100, Loss: 0.12526701352695685, MSE: 0.2505340270539137\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48471785  0.71242267]\n",
      " [ 0.04790663 -0.37949378]\n",
      " [-0.18290337 -0.15242916]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.46418939  0.10627397 -0.10157588]]\n",
      "\n",
      "Epoch 29/100, Loss: 0.12522293797259776, MSE: 0.2504458759451955\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48487581  0.71249142]\n",
      " [ 0.047868   -0.37954576]\n",
      " [-0.18286389 -0.15238827]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.46240949  0.10490672 -0.10296401]]\n",
      "\n",
      "Epoch 30/100, Loss: 0.12518272712843595, MSE: 0.2503654542568719\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48503964  0.71256617]\n",
      " [ 0.04783157 -0.37959539]\n",
      " [-0.18282551 -0.15234847]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.46071505  0.10360143 -0.10429017]]\n",
      "\n",
      "Epoch 31/100, Loss: 0.125146043229009, MSE: 0.250292086458018\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48520905  0.71264662]\n",
      " [ 0.0477972  -0.37964284]\n",
      " [-0.18278825 -0.15230976]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.45910231  0.10235533 -0.10555709]]\n",
      "\n",
      "Epoch 32/100, Loss: 0.12511257731486844, MSE: 0.2502251546297369\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48538376  0.71273249]\n",
      " [ 0.04776477 -0.37968824]\n",
      " [-0.18275207 -0.15227214]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.45756765  0.10116581 -0.1067674 ]]\n",
      "\n",
      "Epoch 33/100, Loss: 0.12508204689731475, MSE: 0.2501640937946295\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48556351  0.71282351]\n",
      " [ 0.04773415 -0.3797317 ]\n",
      " [-0.18271698 -0.15223558]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.45610761  0.10003033 -0.10792359]]\n",
      "\n",
      "Epoch 34/100, Loss: 0.12505419379130633, MSE: 0.25010838758261267\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48574805  0.71291943]\n",
      " [ 0.04770524 -0.37977335]\n",
      " [-0.18268295 -0.15220008]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.45471887  0.09894647 -0.10902809]]\n",
      "\n",
      "Epoch 35/100, Loss: 0.12502878210824284, MSE: 0.2500575642164857\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48593713  0.71302   ]\n",
      " [ 0.04767791 -0.3798133 ]\n",
      " [-0.18264997 -0.15216561]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.45339825  0.09791193 -0.1100832 ]]\n",
      "\n",
      "Epoch 36/100, Loss: 0.12500559639997488, MSE: 0.25001119279994977\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48613055  0.71312501]\n",
      " [ 0.04765209 -0.37985165]\n",
      " [-0.18261801 -0.15213215]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.45214271  0.09692448 -0.11109113]]\n",
      "\n",
      "Epoch 37/100, Loss: 0.12498443994524445, MSE: 0.2499688798904889\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48632808  0.71323423]\n",
      " [ 0.04762767 -0.37988849]\n",
      " [-0.18258705 -0.15209969]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.45094933  0.09598201 -0.11205399]]\n",
      "\n",
      "Epoch 38/100, Loss: 0.12496513316977385, MSE: 0.2499302663395477\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48652954  0.71334746]\n",
      " [ 0.04760458 -0.37992391]\n",
      " [-0.18255708 -0.15206821]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44981533  0.09508249 -0.11297382]]\n",
      "\n",
      "Epoch 39/100, Loss: 0.12494751219135278, MSE: 0.24989502438270556\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48673473  0.71346452]\n",
      " [ 0.04758272 -0.379958  ]\n",
      " [-0.18252806 -0.15203766]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44873804  0.09422397 -0.11385255]]\n",
      "\n",
      "Epoch 40/100, Loss: 0.12493142748150114, MSE: 0.24986285496300228\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48694347  0.71358522]\n",
      " [ 0.04756203 -0.37999083]\n",
      " [-0.18249997 -0.15200805]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44771491  0.09340459 -0.11469203]]\n",
      "\n",
      "Epoch 41/100, Loss: 0.12491674263557973, MSE: 0.24983348527115945\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4871556   0.71370938]\n",
      " [ 0.04754244 -0.38002248]\n",
      " [-0.18247279 -0.15197933]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44674351  0.0926226  -0.11549403]]\n",
      "\n",
      "Epoch 42/100, Loss: 0.12490333324356284, MSE: 0.24980666648712568\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48737095  0.71383685]\n",
      " [ 0.04752389 -0.38005302]\n",
      " [-0.18244649 -0.15195148]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4458215   0.09187628 -0.11626026]]\n",
      "\n",
      "Epoch 43/100, Loss: 0.12489108585406432, MSE: 0.24978217170812864\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48758938  0.71396746]\n",
      " [ 0.0475063  -0.3800825 ]\n",
      " [-0.18242104 -0.15192448]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44494666  0.09116402 -0.11699233]]\n",
      "\n",
      "Epoch 44/100, Loss: 0.12487989702460014, MSE: 0.2497597940492003\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48781075  0.71410109]\n",
      " [ 0.04748963 -0.38011099]\n",
      " [-0.18239641 -0.15189831]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44411686  0.09048426 -0.11769178]]\n",
      "\n",
      "Epoch 45/100, Loss: 0.1248696724514777, MSE: 0.2497393449029554\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48803491  0.71423757]\n",
      " [ 0.04747383 -0.38013855]\n",
      " [-0.18237259 -0.15187292]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44333006  0.08983554 -0.11836011]]\n",
      "\n",
      "Epoch 46/100, Loss: 0.1248603261731035, MSE: 0.249720652346207\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48826175  0.7143768 ]\n",
      " [ 0.04745884 -0.38016523]\n",
      " [-0.18234954 -0.15184831]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44258433  0.08921643 -0.11899872]]\n",
      "\n",
      "Epoch 47/100, Loss: 0.12485177984090327, MSE: 0.24970355968180655\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48849114  0.71451863]\n",
      " [ 0.04744461 -0.38019107]\n",
      " [-0.18232724 -0.15182444]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44187779  0.08862558 -0.11960897]]\n",
      "\n",
      "Epoch 48/100, Loss: 0.12484396205243678, MSE: 0.24968792410487356\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48872296  0.71466295]\n",
      " [ 0.0474311  -0.38021613]\n",
      " [-0.18230567 -0.15180129]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44120869  0.08806171 -0.12019215]]\n",
      "\n",
      "Epoch 49/100, Loss: 0.1248368077416674, MSE: 0.2496736154833348\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48895711  0.71480966]\n",
      " [ 0.04741828 -0.38024045]\n",
      " [-0.18228479 -0.15177883]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.44057533  0.08752358 -0.12074949]]\n",
      "\n",
      "Epoch 50/100, Loss: 0.12483025762170757, MSE: 0.24966051524341515\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48919349  0.71495863]\n",
      " [ 0.0474061  -0.38026406]\n",
      " [-0.18226458 -0.15175704]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43997609  0.08701001 -0.12128217]]\n",
      "\n",
      "Epoch 51/100, Loss: 0.12482425767570585, MSE: 0.2496485153514117\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.489432    0.71510979]\n",
      " [ 0.04739453 -0.38028701]\n",
      " [-0.18224502 -0.15173589]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43940942  0.08651989 -0.12179131]]\n",
      "\n",
      "Epoch 52/100, Loss: 0.12481875869186596, MSE: 0.2496375173837319\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48967254  0.71526302]\n",
      " [ 0.04738353 -0.38030934]\n",
      " [-0.18222608 -0.15171537]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43887386  0.08605214 -0.122278  ]]\n",
      "\n",
      "Epoch 53/100, Loss: 0.12481371583889762, MSE: 0.24962743167779525\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.48991503  0.71541823]\n",
      " [ 0.04737308 -0.38033107]\n",
      " [-0.18220775 -0.15169544]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43836799  0.08560573 -0.12274325]]\n",
      "\n",
      "Epoch 54/100, Loss: 0.12480908827848552, MSE: 0.24961817655697105\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49015938  0.71557535]\n",
      " [ 0.04736313 -0.38035224]\n",
      " [-0.18218999 -0.15167609]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43789046  0.08517969 -0.12318804]]\n",
      "\n",
      "Epoch 55/100, Loss: 0.12480483881163404, MSE: 0.24960967762326808\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49040552  0.7157343 ]\n",
      " [ 0.04735367 -0.38037288]\n",
      " [-0.18217279 -0.15165729]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43744     0.08477309 -0.12361332]]\n",
      "\n",
      "Epoch 56/100, Loss: 0.12480093355599695, MSE: 0.2496018671119939\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49065337  0.71589498]\n",
      " [ 0.04734467 -0.38039302]\n",
      " [-0.18215612 -0.15163902]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43701537  0.08438503 -0.12401997]]\n",
      "\n",
      "Epoch 57/100, Loss: 0.12479734165153547, MSE: 0.24959468330307094\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49090287  0.71605733]\n",
      " [ 0.0473361  -0.38041268]\n",
      " [-0.18213997 -0.15162127]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43661542  0.08401466 -0.12440884]]\n",
      "\n",
      "Epoch 58/100, Loss: 0.12479403499206593, MSE: 0.24958806998413186\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49115393  0.71622129]\n",
      " [ 0.04732794 -0.38043189]\n",
      " [-0.18212431 -0.151604  ]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43623901  0.08366118 -0.12478076]]\n",
      "\n",
      "Epoch 59/100, Loss: 0.12479098798045947, MSE: 0.24958197596091894\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49140651  0.71638678]\n",
      " [ 0.04732017 -0.38045068]\n",
      " [-0.18210912 -0.1515872 ]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43588509  0.08332381 -0.1251365 ]]\n",
      "\n",
      "Epoch 60/100, Loss: 0.12478817730544334, MSE: 0.24957635461088667\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49166054  0.71655374]\n",
      " [ 0.04731276 -0.38046906]\n",
      " [-0.18209438 -0.15157086]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43555264  0.0830018  -0.12547679]]\n",
      "\n",
      "Epoch 61/100, Loss: 0.12478558173812465, MSE: 0.2495711634762493\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49191595  0.71672211]\n",
      " [ 0.0473057  -0.38048705]\n",
      " [-0.18208008 -0.15155495]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43524069  0.08269445 -0.12580235]]\n",
      "\n",
      "Epoch 62/100, Loss: 0.12478318194651625, MSE: 0.2495663638930325\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49217271  0.71689184]\n",
      " [ 0.04729898 -0.38050469]\n",
      " [-0.18206619 -0.15153945]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4349483   0.0824011  -0.12611386]]\n",
      "\n",
      "Epoch 63/100, Loss: 0.1247809603264913, MSE: 0.2495619206529826\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49243075  0.71706286]\n",
      " [ 0.04729256 -0.38052198]\n",
      " [-0.18205271 -0.15152435]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4346746   0.08212108 -0.12641196]]\n",
      "\n",
      "Epoch 64/100, Loss: 0.12477890084772639, MSE: 0.24955780169545277\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49269002  0.71723514]\n",
      " [ 0.04728643 -0.38053894]\n",
      " [-0.18203961 -0.15150964]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43441873  0.08185381 -0.12669726]]\n",
      "\n",
      "Epoch 65/100, Loss: 0.12477698891331734, MSE: 0.2495539778266347\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49295049  0.71740862]\n",
      " [ 0.04728059 -0.3805556 ]\n",
      " [-0.18202687 -0.15149529]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43417991  0.08159868 -0.12697035]]\n",
      "\n",
      "Epoch 66/100, Loss: 0.12477521123186447, MSE: 0.24955042246372894\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49321209  0.71758325]\n",
      " [ 0.04727501 -0.38057196]\n",
      " [-0.18201449 -0.15148128]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43395734  0.08135513 -0.12723179]]\n",
      "\n",
      "Epoch 67/100, Loss: 0.12477355570092888, MSE: 0.24954711140185776\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49347481  0.717759  ]\n",
      " [ 0.04726968 -0.38058804]\n",
      " [-0.18200244 -0.15146762]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4337503   0.08112264 -0.12748213]]\n",
      "\n",
      "Epoch 68/100, Loss: 0.12477201130085658, MSE: 0.24954402260171316\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49373858  0.71793581]\n",
      " [ 0.0472646  -0.38060386]\n",
      " [-0.18199071 -0.15145427]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4335581   0.0809007  -0.12772186]]\n",
      "\n",
      "Epoch 69/100, Loss: 0.12477056799805296, MSE: 0.24954113599610592\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49400338  0.71811365]\n",
      " [ 0.04725973 -0.38061942]\n",
      " [-0.18197929 -0.15144123]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43338005  0.08068882 -0.12795148]]\n",
      "\n",
      "Epoch 70/100, Loss: 0.12476921665687302, MSE: 0.24953843331374603\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49426916  0.71829248]\n",
      " [ 0.04725508 -0.38063475]\n",
      " [-0.18196816 -0.15142849]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43321552  0.08048654 -0.12817145]]\n",
      "\n",
      "Epoch 71/100, Loss: 0.12476794895936294, MSE: 0.24953589791872588\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4945359   0.71847227]\n",
      " [ 0.04725064 -0.38064985]\n",
      " [-0.18195732 -0.15141602]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43306391  0.08029341 -0.12838222]]\n",
      "\n",
      "Epoch 72/100, Loss: 0.12476675733215745, MSE: 0.2495335146643149\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49480356  0.71865298]\n",
      " [ 0.04724639 -0.38066474]\n",
      " [-0.18194674 -0.15140383]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43292463  0.08010902 -0.1285842 ]]\n",
      "\n",
      "Epoch 73/100, Loss: 0.12476563487989745, MSE: 0.2495312697597949\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49507211  0.71883458]\n",
      " [ 0.04724232 -0.38067943]\n",
      " [-0.18193642 -0.15139189]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43279713  0.07993295 -0.12877782]]\n",
      "\n",
      "Epoch 74/100, Loss: 0.1247645753245891, MSE: 0.2495291506491782\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49534152  0.71901704]\n",
      " [ 0.04723842 -0.38069392]\n",
      " [-0.18192635 -0.15138019]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43268087  0.07976484 -0.12896344]]\n",
      "\n",
      "Epoch 75/100, Loss: 0.12476357295037599, MSE: 0.24952714590075198\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49561176  0.71920032]\n",
      " [ 0.04723469 -0.38070822]\n",
      " [-0.18191651 -0.15136873]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43257536  0.0796043  -0.12914144]]\n",
      "\n",
      "Epoch 76/100, Loss: 0.1247626225532429, MSE: 0.2495252451064858\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4958828   0.71938441]\n",
      " [ 0.04723111 -0.38072235]\n",
      " [-0.18190689 -0.15135749]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4324801   0.079451   -0.12931217]]\n",
      "\n",
      "Epoch 77/100, Loss: 0.12476171939521193, MSE: 0.24952343879042385\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49615462  0.71956927]\n",
      " [ 0.04722769 -0.38073632]\n",
      " [-0.18189749 -0.15134647]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43239465  0.07930459 -0.12947597]]\n",
      "\n",
      "Epoch 78/100, Loss: 0.12476085916263138, MSE: 0.24952171832526276\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4964272   0.71975487]\n",
      " [ 0.0472244  -0.38075013]\n",
      " [-0.18188829 -0.15133565]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43231856  0.07916476 -0.12963315]]\n",
      "\n",
      "Epoch 79/100, Loss: 0.12476003792819232, MSE: 0.24952007585638464\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49670051  0.71994121]\n",
      " [ 0.04722125 -0.38076379]\n",
      " [-0.18187929 -0.15132502]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43225141  0.0790312  -0.12978402]]\n",
      "\n",
      "Epoch 80/100, Loss: 0.12475925211634051, MSE: 0.24951850423268102\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49697453  0.72012824]\n",
      " [ 0.04721822 -0.3807773 ]\n",
      " [-0.18187047 -0.15131458]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.4321928   0.07890363 -0.12992887]]\n",
      "\n",
      "Epoch 81/100, Loss: 0.12475849847178085, MSE: 0.2495169969435617\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49724924  0.72031595]\n",
      " [ 0.04721532 -0.38079069]\n",
      " [-0.18186183 -0.15130432]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43214236  0.07878177 -0.13006798]]\n",
      "\n",
      "Epoch 82/100, Loss: 0.12475777403079868, MSE: 0.24951554806159737\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49752461  0.72050432]\n",
      " [ 0.04721253 -0.38080394]\n",
      " [-0.18185335 -0.15129422]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43209971  0.07866535 -0.13020162]]\n",
      "\n",
      "Epoch 83/100, Loss: 0.12475707609514614, MSE: 0.24951415219029227\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49780064  0.72069333]\n",
      " [ 0.04720985 -0.38081707]\n",
      " [-0.18184504 -0.15128429]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43206451  0.07855412 -0.13033004]]\n",
      "\n",
      "Epoch 84/100, Loss: 0.12475640220826464, MSE: 0.2495128044165293\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4980773   0.72088296]\n",
      " [ 0.04720727 -0.38083009]\n",
      " [-0.18183688 -0.15127451]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43203643  0.07844785 -0.13045348]]\n",
      "\n",
      "Epoch 85/100, Loss: 0.12475575013363513, MSE: 0.24951150026727026\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49835457  0.72107319]\n",
      " [ 0.04720479 -0.38084299]\n",
      " [-0.18182887 -0.15126487]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43201516  0.07834629 -0.13057217]]\n",
      "\n",
      "Epoch 86/100, Loss: 0.1247551178350654, MSE: 0.2495102356701308\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49863244  0.721264  ]\n",
      " [ 0.0472024  -0.38085579]\n",
      " [-0.181821   -0.15125538]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43200038  0.07824925 -0.13068632]]\n",
      "\n",
      "Epoch 87/100, Loss: 0.12475450345874214, MSE: 0.2495090069174843\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4989109   0.72145539]\n",
      " [ 0.0472001  -0.38086849]\n",
      " [-0.18181326 -0.15124601]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43199182  0.07815649 -0.13079616]]\n",
      "\n",
      "Epoch 88/100, Loss: 0.12475390531688921, MSE: 0.24950781063377842\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49918992  0.72164733]\n",
      " [ 0.04719789 -0.3808811 ]\n",
      " [-0.18180564 -0.15123678]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43198919  0.07806783 -0.13090188]]\n",
      "\n",
      "Epoch 89/100, Loss: 0.12475332187288904, MSE: 0.24950664374577808\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.4994695   0.7218398 ]\n",
      " [ 0.04719575 -0.38089362]\n",
      " [-0.18179815 -0.15122766]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43199224  0.07798308 -0.13100367]]\n",
      "\n",
      "Epoch 90/100, Loss: 0.12475275172773634, MSE: 0.24950550345547268\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.49974962  0.72203281]\n",
      " [ 0.04719369 -0.38090605]\n",
      " [-0.18179077 -0.15121866]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43200072  0.07790205 -0.13110172]]\n",
      "\n",
      "Epoch 91/100, Loss: 0.12475219360770456, MSE: 0.24950438721540913\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50003027  0.72222632]\n",
      " [ 0.0471917  -0.3809184 ]\n",
      " [-0.1817835  -0.15120977]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43201438  0.07782458 -0.13119619]]\n",
      "\n",
      "Epoch 92/100, Loss: 0.12475164635311692, MSE: 0.24950329270623384\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50031143  0.72242033]\n",
      " [ 0.04718978 -0.38093067]\n",
      " [-0.18177633 -0.15120098]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.432033    0.07775048 -0.13128726]]\n",
      "\n",
      "Epoch 93/100, Loss: 0.12475110890812313, MSE: 0.24950221781624626\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.5005931   0.72261483]\n",
      " [ 0.04718793 -0.38094287]\n",
      " [-0.18176926 -0.15119229]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43205636  0.07767962 -0.13137508]]\n",
      "\n",
      "Epoch 94/100, Loss: 0.12475058031139193, MSE: 0.24950116062278385\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50087526  0.7228098 ]\n",
      " [ 0.04718613 -0.380955  ]\n",
      " [-0.18176228 -0.1511837 ]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43208426  0.07761184 -0.1314598 ]]\n",
      "\n",
      "Epoch 95/100, Loss: 0.12475005968763722, MSE: 0.24950011937527444\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50115791  0.72300523]\n",
      " [ 0.0471844  -0.38096707]\n",
      " [-0.1817554  -0.15117519]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43211649  0.07754699 -0.13154158]]\n",
      "\n",
      "Epoch 96/100, Loss: 0.12474954623990338, MSE: 0.24949909247980676\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50144102  0.72320112]\n",
      " [ 0.04718271 -0.38097907]\n",
      " [-0.18174859 -0.15116677]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43215287  0.07748494 -0.13162053]]\n",
      "\n",
      "Epoch 97/100, Loss: 0.1247490392425418, MSE: 0.2494980784850836\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50172461  0.72339746]\n",
      " [ 0.04718109 -0.38099102]\n",
      " [-0.18174187 -0.15115843]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43219322  0.07742556 -0.13169681]]\n",
      "\n",
      "Epoch 98/100, Loss: 0.12474853803481652, MSE: 0.24949707606963303\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50200864  0.72359423]\n",
      " [ 0.04717951 -0.3810029 ]\n",
      " [-0.18173522 -0.15115016]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43223736  0.07736872 -0.13177052]]\n",
      "\n",
      "Epoch 99/100, Loss: 0.12474804201508316, MSE: 0.24949608403016632\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50229313  0.72379142]\n",
      " [ 0.04717797 -0.38101474]\n",
      " [-0.18172865 -0.15114197]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43228514  0.0773143  -0.1318418 ]]\n",
      "\n",
      "Epoch 100/100, Loss: 0.12474755063548928, MSE: 0.24949510127097857\n",
      "Weights:\n",
      "Layer 1:\n",
      "W1:\n",
      "[[ 0.50257805  0.72398903]\n",
      " [ 0.04717649 -0.38102652]\n",
      " [-0.18172214 -0.15113384]]\n",
      "Layer 2:\n",
      "W2:\n",
      "[[ 0.43233639  0.0772622  -0.13191075]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "layers_dim = [2, 3, 1]  # Example: 2 input units, 3 units in the hidden layer, 1 output unit\n",
    "activation_func = SigmoidActivation  # Use Sigmoid activation function\n",
    "loss = L2Loss()  # Use L2 loss function\n",
    "model = Model(layers_dim, activation_func, loss)\n",
    "\n",
    "# Assuming X and y are your input and output data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "num_epochs = 100  # Specify the number of epochs\n",
    "learning_rate = 0.1  # Specify the learning rate\n",
    "model.train(X.T, y.T, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gates w msh 3arf ash8lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the base class for Gates\n",
    "class Gate:\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Example of an AddGate class inheriting from the Gate class\n",
    "class AddGate(Gate):\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x + y\n",
    "\n",
    "    def backward(self, dz):\n",
    "        dx = dz * np.ones_like(self.x)\n",
    "        dy = dz * np.ones_like(self.y)\n",
    "        return dx, dy\n",
    "\n",
    "# Example of a MultiplyGate class inheriting from the Gate class\n",
    "class MultiplyGate(Gate):\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "\n",
    "    def backward(self, dz):\n",
    "        dx = dz * self.y\n",
    "        dy = dz * self.x\n",
    "        return dx, dy\n",
    "\n",
    "# Define activation functions\n",
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "# Define loss functions\n",
    "class LossFunctions:\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_true, y_pred):\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_loss(y_true, y_pred):\n",
    "        return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "# Define the Model class\n",
    "class Model:\n",
    "    def __init__(self, layers_dim, activation_func, loss):\n",
    "        self.layers_dim = layers_dim\n",
    "        self.activation_func = activation_func\n",
    "        self.loss = loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        z1 = MultiplyGate().forward(X, np.array([[1], [1]]))  # AND gate\n",
    "        a1 = ActivationFunctions.relu(z1)\n",
    "        z2 = MultiplyGate().forward(a1, np.array([[2]]))  # Output layer\n",
    "        y_pred = ActivationFunctions.sigmoid(z2)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Forward pass\n",
    "            z1 = MultiplyGate().forward(X, np.array([[1], [1]]))  # AND gate\n",
    "            a1 = ActivationFunctions.relu(z1)\n",
    "            z2 = MultiplyGate().forward(a1, np.array([[2]]))  # Output layer\n",
    "            y_pred = ActivationFunctions.sigmoid(z2)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.loss(y, y_pred)\n",
    "\n",
    "            # Backward pass\n",
    "            dz2 = y_pred - y\n",
    "            dz2 *= y_pred * (1 - y_pred)\n",
    "            da1, dw2 = MultiplyGate().backward(dz2)\n",
    "            dz1 = da1 * (a1 > 0)\n",
    "            dx, dw1 = MultiplyGate().backward(dz1)\n",
    "\n",
    "            # Update weights\n",
    "            learning_rate = 0.01\n",
    "            dw1 *= learning_rate\n",
    "            dw2 *= learning_rate\n",
    "\n",
    "            # Print loss\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
